{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook for first canonical problem for multimodal optimization looking for localized extreme\n",
    "# events with agent-based military simulation\n",
    "# Experiment 2.1 - Deterministic literature test problem \"uneven decreasing maxima\" [5]\n",
    "# Part 1 - balanced strategy (PIONEER)\n",
    "#\n",
    "# Author: Alex Braafladt\n",
    "# Initial creation: 3/11/23\n",
    "#\n",
    "# Goal: Benchmark state-of-the-art Bayesian Optimization approaches (and QMC/MCS) on a multimodal\n",
    "#       optimization test function focused on localized extrema\n",
    "#\n",
    "# Notes:\n",
    "# -Using the BoTorch framework [1] and NEI from [4]\n",
    "# -TEAD technique from [2]\n",
    "# -TuRBO technique from [3]\n",
    "#\n",
    "# References:\n",
    "# [1] M. Balandat et al., “BOTORCH: A framework for efficient Monte-Carlo Bayesian optimization,”\n",
    "#     Adv. Neural Inf. Process. Syst., vol. 2020-Decem, no. MC, 2020.\n",
    "# [2] S. Mo et al., “A Taylor Expansion-Based Adaptive Design Strategy for Global Surrogate\n",
    "#     Modeling With Applications in Groundwater Modeling,” Water Resour. Res., vol. 53, no.\n",
    "#     12, pp. 10802–10823, 2017, doi: 10.1002/2017WR021622.\n",
    "# [3] D. Eriksson, M. Pearce, J. R. Gardner, R. Turner, and M. Poloczek, “Scalable global\n",
    "#     optimization via local Bayesian optimization,” Adv. Neural Inf. Process. Syst., vol.\n",
    "#     32, no. NeurIPS, 2019.\n",
    "# [4] B. Letham, B. Karrer, G. Ottoni, and E. Bakshy, “Constrained Bayesian optimization with noisy\n",
    "#     experiments,” Bayesian Anal., vol. 14, no. 2, pp. 495–519, 2019, doi: 10.1214/18-BA1110.\n",
    "# [5] X. Li, A. Engelbrecht, and M. G. Epitropakis, “Benchmark Functions for CEC’2013 Special\n",
    "#     Session and Competition on Niching Methods for Multimodal Function Optimization,” 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from extremasearch.globalmm.globalsearch import MultimodalExtremaSearch\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "import time\n",
    "\n",
    "\n",
    "# setup\n",
    "dtype = torch.double\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import warnings\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup file i/o\n",
    "import os as os\n",
    "import datetime as dt\n",
    "# get current working directory\n",
    "wrkdir = os.getcwd()\n",
    "print('Current working directory: '+wrkdir)\n",
    "# set up a data save directory for all future runs\n",
    "newoutputdir = wrkdir+'\\output'\n",
    "if not os.path.exists(newoutputdir):\n",
    "    os.makedirs(newoutputdir)\n",
    "# set up a new directory to store files for the current run - updates at each new full run of notebook\n",
    "curDatetime = dt.datetime.now()\n",
    "datasavedir = newoutputdir + r'\\\\' + 'exp2.1_balanced_lit2' + str(curDatetime.strftime('%Y%m%d%H%M%S'))\n",
    "if not os.path.exists(datasavedir):\n",
    "    os.makedirs(datasavedir)\n",
    "print('Data save directory: '+datasavedir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test function\n",
    "def udm_torch(x):\n",
    "    x = x\n",
    "    y = torch.zeros(x.shape)\n",
    "    y += torch.exp(-2.0*torch.log(torch.tensor(2.0))*((x - 0.08)/0.854)**2.0) * torch.sin(5.0*torch.pi*(x**(3.0/4.0)-0.05))**6.0\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the test function\n",
    "x1 = torch.linspace(0,1.0,200)\n",
    "y1 = udm_torch(x1)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "sns.lineplot(x=x1, y=y1, ax=ax, color='r')\n",
    "# ax.set_xlim([0.9, 1.0])\n",
    "# ax.set_ylim([0.2, 0.35])\n",
    "plt.savefig(datasavedir + '/'+'deterministic_test_problem'+'.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wrapper for use in optimization loop\n",
    "def outcome_objective(x):\n",
    "    \"\"\"wrapper for the outcome objective function\"\"\"\n",
    "    return udm_torch(x).type_as(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# functions for getting and working with data from searches\n",
    "def get_bounds(graph):\n",
    "    \"\"\"Get the bounds for each node in a graph to be able to plot them\"\"\"\n",
    "    graph_leaves = [n for n in graph if graph.out_degree[n] == 0]\n",
    "    bound_list = []\n",
    "    node_list = []\n",
    "    for n in graph_leaves:\n",
    "        # get current leaf node\n",
    "        current_node = graph.nodes()[n]\n",
    "        current_state = current_node['data']\n",
    "        current_bounds = current_state.local_bounds\n",
    "        node_list.append(n)\n",
    "        bound_list.append(current_bounds)\n",
    "    return node_list, bound_list\n",
    "\n",
    "\n",
    "def get_bounds_all(graph):\n",
    "    \"\"\"Get the bounds for each node in a graph to be able to plot them\"\"\"\n",
    "    graph_nodes = [n for n in graph]\n",
    "    bound_list = []\n",
    "    node_list = []\n",
    "    for n in graph_nodes:\n",
    "        # get current node\n",
    "        current_node = graph.nodes()[n]\n",
    "        current_state = current_node['data']\n",
    "        current_bounds = current_state.local_bounds\n",
    "        node_list.append(n)\n",
    "        bound_list.append(current_bounds)\n",
    "    return node_list, bound_list\n",
    "\n",
    "\n",
    "def fit_local_models(graph):\n",
    "    \"\"\"Go through leaf nodes and update LocalSearchState to have local {x, y}\n",
    "    based on global {x, y} that are within bounds\"\"\"\n",
    "    graph_leaves = [n for n in graph if graph.out_degree[n] == 0]\n",
    "    for n in graph_leaves:\n",
    "        current_node = graph.nodes()[n]\n",
    "        current_state = current_node['data']\n",
    "        current_state.local_model = SingleTaskGP(current_state.x_local, current_state.y_local,\n",
    "                                                 input_transform=Normalize(d=current_state.x_local.shape[-1]),\n",
    "                                                 outcome_transform=Standardize(m=current_state.y_local.shape[-1]))\n",
    "        current_state.local_mll = ExactMarginalLogLikelihood(current_state.local_model.likelihood,\n",
    "                                                             current_state.local_model)\n",
    "        fit_gpytorch_mll(current_state.local_mll)\n",
    "\n",
    "\n",
    "def plot_local_models_1d(graph, x_search, y_search, rep):\n",
    "    \"\"\"Go through leaf nodes and plot the local models only within the bounds of the node\"\"\"\n",
    "    fit_local_models(graph)\n",
    "    nodes, bnds = get_bounds(graph)\n",
    "    x_test = torch.linspace(0, 1, 400)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    # true objective\n",
    "    ax.plot(x_test.numpy(), outcome_objective(x_test).numpy(), 'r-', alpha=0.9, label='true objective')\n",
    "    # local models\n",
    "    graph_leaves = [n for n in graph if graph.out_degree[n] == 0]\n",
    "    for n in graph_leaves:\n",
    "        current_node = graph.nodes()[n]\n",
    "        current_state = current_node['data']\n",
    "        current_model = current_state.local_model\n",
    "        current_bounds = current_state.local_bounds\n",
    "        current_x_test_mask = torch.ge(x_test, current_bounds[0]) & torch.lt(x_test, current_bounds[1])\n",
    "        current_x_test = x_test[current_x_test_mask]\n",
    "        mean_test = current_model.posterior(current_x_test.unsqueeze(-1)).mean.detach().numpy()\n",
    "        ax.plot(current_x_test.numpy(), mean_test, 'b-', alpha=0.9, label='surrogate mean')\n",
    "        var_test = current_model.posterior(current_x_test.unsqueeze(-1)).variance.detach().numpy()\n",
    "        sd_test = np.sqrt(var_test)\n",
    "        upper_test = mean_test + 2.0 * sd_test\n",
    "        lower_test = mean_test - 2.0 * sd_test\n",
    "        ax.fill_between(current_x_test.numpy(), lower_test.squeeze(), upper_test.squeeze(), color='b', alpha=0.3,\n",
    "                        label=r'surrogate 2$\\sigma$')\n",
    "    # training points\n",
    "    ax.plot(x_search.numpy()[0:10], y_search[0:10].numpy(), '.', color='tab:orange', label='initial data')\n",
    "    ax.plot(x_search.numpy()[10:], y_search[10:].numpy(), '.', color='g', label='bo data')\n",
    "    # partitions\n",
    "    for n, b in zip(nodes, bnds):\n",
    "        ax.plot([b[0].numpy(), b[0].numpy()], [-0.05, 1.45], 'k-')\n",
    "    # ax.legend()\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    ax.set_title('PIONEER_rep{}'.format(rep))\n",
    "    # ax.set_xlim([0.915, 0.925])\n",
    "    plt.savefig(datasavedir + '/'+'1d_results_local_models_rep{}'.format(rep)+'.png')\n",
    "\n",
    "\n",
    "def plot_current_tree(global_search: MultimodalExtremaSearch, rep):\n",
    "    \"\"\"Save a plot of the tree structure after the search\"\"\"\n",
    "    # tree figure\n",
    "    T = global_search.global_state.partition_graph\n",
    "    pos = nx.nx_agraph.graphviz_layout(T, prog=\"dot\")\n",
    "    # bounds to put on nodes on plot\n",
    "    all_nodes, all_bounds = get_bounds_all(global_search.global_state.partition_graph)\n",
    "    # positioning of bound text\n",
    "    pos_attrs = {}\n",
    "    for node, coords in pos.items():\n",
    "        pos_attrs[node] = (coords[0]-15, coords[1])\n",
    "    custom_node_attrs = {}\n",
    "    for node, attrs in zip(all_nodes, all_bounds):\n",
    "        custom_node_attrs[node] = str(attrs.numpy().round(decimals=3))\n",
    "    # plot commands\n",
    "    f, ax = plt.subplots(1, 1, figsize=(12, 8), constrained_layout=True)\n",
    "    nx.draw(T, pos, with_labels=True, font_weight='bold', ax=ax)\n",
    "    nx.draw_networkx_labels(T, pos_attrs, labels=custom_node_attrs, ax=ax)\n",
    "    ax.set_title('PIONEER Partition Tree')\n",
    "    plt.savefig(datasavedir + '/'+'partition_tree_rep{}'.format(rep)+'.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# experiment functions to calculate metrics\n",
    "# metrics functions\n",
    "def count_number_peaks_observed(x_obs, y_obs, num_known_peaks=5):\n",
    "    \"\"\"Function to count the number of peaks observed for the mme_noise_jump_1d function\"\"\"\n",
    "    peak1, peak2, peak3, peak4, peak5 = False, False, False, False, False\n",
    "    for x, y in zip(x_obs, y_obs):\n",
    "        if 0.003 <= x <= 0.013:\n",
    "            if y >= 0.95:\n",
    "                peak1 = True\n",
    "        elif 0.240 <= x <= 0.250:\n",
    "            if y >= 0.90:\n",
    "                peak2 = True\n",
    "        elif 0.445 <= x <= 0.455:\n",
    "            if y >= 0.73:\n",
    "                peak3 = True\n",
    "        elif 0.673 <= x <= 0.683:\n",
    "            if y >= 0.47:\n",
    "                peak4 = True\n",
    "        elif 0.925 <= x <= 0.935:\n",
    "            if y >= 0.238:\n",
    "                peak5 = True\n",
    "    num_peaks_observed = peak1 + peak2 + peak3 + peak4 + peak5\n",
    "    return num_peaks_observed\n",
    "\n",
    "\n",
    "def count_evaluations_for_all_peaks(x_obs, y_obs):\n",
    "    \"\"\"Count the number of function evaluations before finding all peaks\"\"\"\n",
    "    i = 0\n",
    "    num_evals_for_all_peaks = x_obs.shape[0]\n",
    "    peak1, peak2, peak3, peak4, peak5 = False, False, False, False, False\n",
    "    for x, y in zip(x_obs, y_obs):\n",
    "        if 0.003 <= x <= 0.013:\n",
    "            if y >= 0.95:\n",
    "                peak1 = True\n",
    "        elif 0.240 <= x <= 0.250:\n",
    "            if y >= 0.90:\n",
    "                peak2 = True\n",
    "        elif 0.445 <= x <= 0.455:\n",
    "            if y >= 0.73:\n",
    "                peak3 = True\n",
    "        elif 0.673 <= x <= 0.683:\n",
    "            if y >= 0.47:\n",
    "                peak4 = True\n",
    "        elif 0.925 <= x <= 0.935:\n",
    "            if y >= 0.238:\n",
    "                peak5 = True\n",
    "        i += 1\n",
    "        if peak1 and peak2 and peak3 and peak4 and peak5:\n",
    "            num_evals_for_all_peaks = i\n",
    "            break\n",
    "    return num_evals_for_all_peaks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# replicated searches\n",
    "\n",
    "# search and experiment settings\n",
    "n_replications = 15\n",
    "global_bounds = torch.tensor([0.0, 1.0], dtype=dtype)\n",
    "n_evals = 130\n",
    "\n",
    "# data holders\n",
    "x_observed_all = []\n",
    "y_observed_all = []\n",
    "distinct_peaks = []\n",
    "function_evaluations = []\n",
    "\n",
    "# experiment loop\n",
    "for rep in range(1, n_replications + 1):\n",
    "    print(f\"\\nTrial {rep:>2} of {n_replications} \", end=\"\")\n",
    "    t0 = time.monotonic()\n",
    "    # initialize search\n",
    "    current_global_search = MultimodalExtremaSearch(60,global_bounds,outcome_objective,total_iteration_limit=n_evals)\n",
    "    # run search\n",
    "    current_global_search.run_global_search()\n",
    "\n",
    "    # collect data\n",
    "    x_observed_all.append(current_global_search.global_state.x_global)\n",
    "    y_observed_all.append(current_global_search.global_state.y_global)\n",
    "\n",
    "    # run and save plots\n",
    "    # results with local model\n",
    "    plot_local_models_1d(current_global_search.global_state.partition_graph,\n",
    "                         current_global_search.global_state.x_global,\n",
    "                         current_global_search.global_state.y_global,\n",
    "                         rep)\n",
    "    # partitioning results tree\n",
    "    plot_current_tree(current_global_search, rep)\n",
    "\n",
    "    # calculate metrics\n",
    "    num_peaks_observed = count_number_peaks_observed(current_global_search.global_state.x_global,\n",
    "                                                     current_global_search.global_state.y_global)\n",
    "    print(\"Observed {num:} peaks\".format(num=num_peaks_observed),)\n",
    "    distinct_peaks.append(num_peaks_observed)\n",
    "    fe = count_evaluations_for_all_peaks(current_global_search.global_state.x_global,\n",
    "                                         current_global_search.global_state.y_global)\n",
    "    print(\"Function evaluations to find all peaks: {}\".format(fe))\n",
    "    function_evaluations.append(fe)\n",
    "    # time for search\n",
    "    t1 = time.monotonic()\n",
    "    print(\"Time to complete search: {}\".format(t1-t0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate summary metrics\n",
    "# peak ratio\n",
    "num_known_peaks = 5\n",
    "# n_replications = 15\n",
    "peak_ratio = sum(distinct_peaks) / (num_known_peaks * n_replications)\n",
    "print(\"Peak ratio: {}\".format(peak_ratio))\n",
    "# success rate\n",
    "num_successes = 0\n",
    "for i in range(n_replications):\n",
    "    if distinct_peaks[i] == num_known_peaks:\n",
    "        num_successes += 1\n",
    "success_ratio = num_successes / n_replications\n",
    "print(\"Success ratio: {}\".format(success_ratio))\n",
    "print(\"Average function evaluations: {}\".format(sum(function_evaluations)/n_replications))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save data to file for potential future use\n",
    "methods_list = ['PIONEER']\n",
    "pr_list = [peak_ratio]\n",
    "sr_list = [success_ratio]\n",
    "fe_list = [function_evaluations]\n",
    "import pickle\n",
    "with open(datasavedir + '/' + 'mme_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump([methods_list, pr_list, sr_list, fe_list], f)\n",
    "# to retrieve\n",
    "# with open(datasavedir + '/' + 'mme_metrics.pkl') as f:\n",
    "#     methods_list, pr_list, sr_list, fe_list = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
